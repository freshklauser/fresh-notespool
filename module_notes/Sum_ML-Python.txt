*********************************20180730********************************************
机器学习
一、机器学习、人工只能与深度学习
人工智能
    机器学习
	经典机器学习（重算法）
	    基于神经网络的机器学习（轻算法，重数据量）
		浅层学习
		深度学习（在已有的规则基础上学习）
	    强化学习（奖惩激励机制促进学习）
	    迁移学习

二、机器学习基本类型
1. 有监督学习：根据已知的输入和输出，建立联系它们的模型，根据该模型对未知输出的输入进行判断
1）回归：以无限连续域的形式表示输出
2）分类：以有限离散域的形式表示输出

2. 无监督学习：在一组没有已知输出（标签）的输入中，根据数据的内部特征和练习，找到某种规则，进行族群的划分---聚类

3. 半监督学习：从一个相对有限的已知结构中利用有监督学习的方法，构建基本模型，通过对未知输入和已知输入的必读判断其输出，扩展原有的已知领域。（知识库的自我扩展）

三、机器学习的基本过程
数据采集 --> 数据清洗 --> 数据预处理 --> 选择模型--> 训练模型
原材料	     去除杂质     准备		 算法	     规则
						       |
						       v
					使用模型 <-- 测试模型
					业务生产     检验

四、数据预处理
	一列一特征
		|
		V
一行一样本 --> /xxxxx\		/yyy\
	       |xxxxx| 样本矩阵 |yyy| 输出矩阵
	       \xxxxx/		\yyy/

姓名	年龄	身高	体重	...
xx	xx	xx	xx	...
xx	xx	xx	xx	...
...

1）均值移除 (数据基线和分散度调整)					(sklearn.preprocessing)
为了统一样本矩阵中不同特恒的基准值和分散度，可以将各个特征的平均值调整为0，标准差调整为1，这个过程称为均值移除。
a	b	c
m = (a+b+c)/3
a-m	b-m	c-m
m' = (...) = (a+b+c)/3 - 3m/3 = 0 (mean == 0)

A	B	C
s = sqrt((A^2+B^2+C^2)/3)
A/s	B/s	C/s
s' = sqrt(...) = 1		  (std == 1)	

sklearn.preprocessing.scale(原始样本矩阵) 
    --> return：均值移除后的样本矩阵(mean=0, std=1)
    def scale(X, axis=0, with_mean=True, with_std=True, copy=True):
    	Standardize a dataset along any axis

2）范围缩放：统一样本矩阵中不同特征的最大值和最小值范围			(sklearn.preprocessing)
sklearn.preprocessing.MinMaxScaler(feature_range=期望最小最大值, copy=True)
    --> return: 范围缩放器
                范围缩放器.fit_transform(原始样本矩阵)
                    --> return：范围缩放后的样本矩阵
    Parameters
    ----------
    feature_range : tuple (min, max), default=(0, 1)
        Desired range of transformed data.
    copy : boolean, optional, default True
        Set to False to perform inplace row normalization and avoid a
        copy (if the input is already a numpy array).
（范围缩放问题：代码中3和4都同时规整为1？？）

3）归一化：								(sklearn.preprocessing)
为了用占比表示特征，用每个样本的特征值除以该样本的特征值绝对值之和，以
使每个样本的特征值绝对值之和为1。 （转化为占比 normalized）
sklearn.preprocessing.normalize(原始样本矩阵，norm='l1')
	--> return：归一化后的样本矩阵
l1即L1范数，矢量中各元素绝对值之和。
l2即L2范数，矢量元素绝对值的平方和再开方
def normalize(X, norm='l2', axis=1, copy=True, return_norm=False):
    norm : 'l1', 'l2', or 'max', optional ('l2' by default)
    The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).

4）二值化：								(sklearn.preprocessing)
用0和1来表示样本矩阵中相对于某个给定阈值高于或低于它的元素	
sklearn.preprocessing.Binarizer(threshold=阈值, copy=True)        # copy default True
    --> return：二值化器
                二值化器.transform(原始样本矩阵) 
                    --> return: 二值化后的样本矩阵 （不可逆过程）--考虑独热编码
threshold: feature <= threshold: feature = 0;
                    > threshold: feature = 1.

5）独热编码 								(sklearn.preprocessing)
1	3	2
7	5	4
1  	8  	6
7  	3  	9
第2列特征有3个数，用3个编码表示
1:10	3:100	2:1000
7:01	5:010	4:0100
	8:001	6:0010
		9:0001
稀疏矩阵：
	101001000
	010100100
	100010010
	011000001
独热编码     (稀疏矩阵) 
sklearn.preprocessing.OneHoteEncoder(sparse=是否采用压缩格式, dtype=元素类型)
    --> return：独热编码器
                独热编码器.fit_transform(原始样本矩阵)
                --> return：独热编码后的样本矩阵，同时构建编码表字典
                独热编码器.transform(原始样本矩阵)
                 --> return：独热编码后的样本矩阵，使用已有的编码表字典
（问题：什么情况下使用？？）

6）标签编码								(sklearn.preprocessing)
不同Series即不同特征的编码表相互独立，编码和解码都要对应使用
将字符形式的特征值映射为整数
sklearn.preprocessing.LabelEncoder()
    --> return：标签编码器
                标签编码器.fit_transform(原始样本矩阵)
                    --> return：编码样本矩阵，构建编码字典
                标签编码器.transform(原始样本矩阵)
                    --> return：编码样本矩阵，使用已有编码表字典
                标签编码器.inverse_transform(编码样本矩阵)
                    --> return：原始样本矩阵，使用编码表字典
返回的标签编码是按照字母顺序前后进行编码的
import sklearn.preprocessing as sp
c = np.array(['vasd', 'sav', 'ba','fd', 'saf', 'vbad'])
model.fit_transform(c)
Out[57]: array([4, 3, 0, 1, 2, 5], dtype=int64)
sorted(c)
Out[58]: ['ba', 'fd', 'saf', 'sav', 'vasd', 'vbad']
sorted(model.transform(c))
Out[59]: [0, 1, 2, 3, 4, 5]

五、线性回归								(sklearn.linear_model)
m个输入样本 -> m个输出标签
	 x1 -> y1
	 x2 -> y2
	 ...
	 xm -> ym
     xk + b -> y 

1.预测函数：联系输出和输入的数学函数。
y= kx + b
其中的k和b称为模型参数，根据已知输入样本和对应的输出标签来训练得出

2.均方误差：每个已知输入样本所对应的实际输出标签和由模型预测出来的输出标签之间的误差平方的平均值
kx1 + b = y1'
kx2 + b = y2'
...
kxm + b = ym'
	(y1-y1')^2 + (y2-y2')^2+...+(ym-ym')^2
error: 	--------------------------------------- = J(k,b) --成本函数
	 		m
		
3.成本函数：将均方误差看做是关于模型参数的函数，谓之成本函数，记做 J(k,b)。
线性回归问题的本质就是寻找能够使成本函数J(k,b)极小值的模型参数
4.梯度下降
loss = J(k,b)
5.接口
sklearn.linear_model.LinearRegression()
    --> return：线性回归器
                线性回归器.fit(输入样本，输出标签)
                线性回归器.predict(输入样本)
                    -- > return：预测输出标签

6）复用
通过pickle将内存中的模型对象写入磁盘文件(pickle.dump(model, f))，或从磁盘文件载入内存(model=pickle.load(f))，以此保存训练好的模型，以备复用
# 模型写入硬盘 pkl格式 方便pickle模块读取
with open('linear.pkl', 'wb') as f:
    pickle.dump(model, f)
# 模型写入硬盘 pkl格式 方便pickle模块读取
with open('linear.pkl', 'rb') as f:
    model = pickle.load(f)
pred_y = model.predict(x)


*********************************20180731********************************************
六、岭回归								(sklearn.linear_model)
岭回归 (削弱异常值对拟合的影响,正则强度越大，削弱的越厉害，降低对异常数据的依赖)
loss = J(k, b) + 正则函数(样本权重)*正则强度(或惩罚系数）
sklearn.linear_model.Ridge(正则强度,
                           fit_intercept=是否修正截距，
                           max_iter=最大迭代次数)
    --> return：岭回归器
                岭回归器.fit()              # 训练数据
                岭回归器.predict()          # 预测数据

七、欠拟合和过拟合
欠拟合：无论是训练数据还是测试数据，模型给出的预测值和真实值都存在较大的误差。
过拟合：模型对于训练数据具有较高的精度，但是对测试数据则表现极差。模型过于特殊，不够泛化（不够一般，即普适性不强）
欠拟合 <--- 模型复杂度 ---> 过拟合

八、多项式回归								(sklearn.linear_model)
        x -> y   y = kx + b
    x x^2 -> y   y = k1x^2 + k2x + b
x x^2 x^3 -> y   y = k1x^3 + k2x^2 + k3x^3 + b
...
sklearn.preprocessing.PolynomialFeatures(最高次数)
    --> return：多项式特征扩展器
sklearn.pipeline.make_pipe(多项式特征扩展器, 线性回归器)              # 管线函数
    --> return：参数k1,k2,k3...
x-->多项式特征扩展器 -- x x^2 x^3 ... --> 线性回归器 ---> k1,k2,k3...

*********************************决策树 sklearn.tree / sklearn.ensemble **********************************************
九、决策树
相似的输入会有相似的输出
――――――――――――――――――――――――――――――――――
输入：				|输出：	回归问题    |	分类问题    |
学历	院校	性别	成绩... | --->  月薪	    |	0-低	    |
0-专科  0-普通	0-女	0-差	|	8000	    |	1-中	    |
1-..	1-...	1-男	1-合格	|	7000	    |	2-高	    |
2-..	2-..	2-..	2-..	|	20000       |		    |
――――――――――――――――――――――――――――――――――
薪资预测：			
输入：				|输出： 月薪		薪资水平(分类)
1	1	0	3	|	？		？
回归---平均
分类---投票
特征工程--优化：
1）结合业务有限选择有限的主要特征，划分子表，降低决策树的高度
   缺点：特征数量多的时候，决策树可能层级过高
2）根据香农定理计算根据每一个特征划分子表前后的信息熵差，选择熵减少量最大的特征，优先参与子表划分；
    信息熵（无序-->有序 熵减小过程；熵减越大，有序性越强）  
    特征选择：选择能使熵减最大化的特征作为主要特征
3）集合算法：根据不同方法，构建多个决策树，利用他们的预测结果，按照区均（回归）或投票（分类）的方法，产生最终的预测结果
A.自助聚合
    采用有放回的抽样规则，从m个样本中随机抽取n个样本，构建一棵决策树，重复以上过程b次，得到b棵决策树。利用每棵决策树的预测结果，根据平均或者投票得到最终预测结果。

B.随机森林(使用较多)：行和列的挑选都是局部化    
    在自助聚合算法的基础上更进一步，对特征也应用自助聚合，即每次训练时，不是用所有的特征构建树结构，而是随机选择部分特征参与构建，以此避免特殊特征对预测结果的影响。

C.正向激励（不断调整样本权重）
    初始化时，针对m个样本分配初始权重，然后根据这个带有权重的模型预测训练样本，针对那些预测错误的样本，提高其权重，再构建一棵决策树模型，重复以上过程，得到b棵决策树。
评估器数：b

单棵决策树：
sklearn.tree.DecisionTreeRegressor()
    --> 决策树回归器
    

正向激励：
sklearn.ensemble.AdaBoostRegressor(元回归器，
                                   n_estimators=评估器数 --b，
                                   random_state=随机种子源  一般选5或7)
    --> 正向激励回归器
random_state:此参数让结果容易复现。 一个确定的随机值将会产生相同的结果，在参数和训练数据不变的情况下.


随机森林：
sklearn.ensemble.RandomForestRegressor(max_depth=最大树高,
                                       n_estimators=评估器数 b,
                                       min_samples_split=划分子表的最小样本数)
min_samples_split: 内部节点再划分所需最小样本数，默认2。这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

    --> 随机森林回归器
代码：houseDecisionTree.py

决策树模型.feature_importances_ ： 特征重要性(根据训练过程中信息熵的计算得到的)
代码：feature_import.py
不同模型和不同数据量的数据集对特征重要性都有不同影响

******************************************************************************************************************
						分类
vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

*********************************简单分类器******************************************
三、简单分类器
输入	输出
3  1	0
2  5	1
1  8    1
6  4 	0
5  2	0
3  5	1
4  7	1
4 -1	0
7  5	？--> 0

******************************逻辑回归分类器 sklearn.linear_model.LogisticRegression**********************************
四、逻辑分类
1. 预测函数
x1 x2 -> y

	 1
y = -----------   (线性输入 --> 非线性输出)
     1 + e^-z
z = k1x1 + k2x2 + b （线性输入 z=0分界，分界线k1x1 + k2x2 + b=0必然为直线）

3. 成本函数
交叉熵误差
J(k1,k2,b) = sigma(-ylog(y')-(1-y)log(1-y'))/m + 
	     m
             正则函数(||k1,k2,b||) * 正则强度

x x --> 0.9(=1的概率) =》1
x x --> 0.2(=1的概率) =》0

sklearn.linear_model.LogisticRegression(solver='liblinear',C=正则强度)



*********************************20180801********************************************

*********************************朴素贝叶斯分类 sklearn.naive_bayes**************************************
五、朴素贝叶斯分类

1. 贝叶斯定理
	    P(A)P(B|A)
P(A|B) = ----------------
	       P(B)

2. 朴素贝叶斯分类
求X样本属于C类别的概率。即当观察到X样本出现时，其所属的类别为C的概率：

P(C|X) = P(C)P(X|C)/P(X)  (P(X)常量)------------# 关键点1：朴素贝叶斯定理
P(C|X) ~ P(C)P(X|C):
P(C)P(X|C) = P(C,X)				# P(C,X)：联合概率，同时发生的概率
	   = P(C,x1,x2,..,xn)			# X的特征（x1,...,xn）同时出现
	   = P(x1,x2,..,xn,C)			# x2,x3,...,xn,C 视为一个整体
	   = P(x1|(x2,..,xn,C))P(x2,..,xn,C)
	   = P(x1|(x2,..,xn,C))p(x2|(x3,..,xn,C))P(x3,..,xn,C)
	   = P(x1|(x2,..,xn,C))p(x2|(x3,..,xn,C))P(x3,..,xn,C)...P(C)
朴素：条件独立假设，即样本各个特征x1,x2,..,xn之间并无关联，不构成条件约束,故可简化为如下：------------------------------------------# 关键点2：朴素，特征无关联
==》	   = P(x1|C)P(x2|C)...P(xn|C)P(C)

X样本属于C类别的概率，正比于C类别出现的概率乘以C类别中各个特征出现的概率的乘积

import sklearn.naive_bayes as nb
代码：nb.py

3. 划分训练集和测试集					(sklearn.model_selection)
sklearn.model_selection.train_test_split(输入集合，
					 输出集合,
					 test_size=测试集占比，
					 random_state=随机种子源)   4份trainxy testxy

4. 交叉验证（评估模型性能）：				(sklearn.model_selection)
1）查准率和召回率
查准率：被正确识别为某类别的样本数 / 被识别为该类别的样本数
	（对不对，反映正确性）
召回率：被正确识别为某类别的样本数 / 该类别的实际样本数
	（全不全，反映完整性）
综合后的评估指标：
f1_score = 2*查准率*召回率 / (查准率+召回率)
sklearn.model_selection.cross_val_score(分类器,
                                        输入集合,
                                        输出集合,
                                        cv =验证次数,        # 交叉验证次数
                                        scoring=验证指标名称)
    --> return:验证指标值数组(size=cv)
0 <---> 1
差	好

5 混淆矩阵						(sklearn.metrics)
以实际类别为行，以预测类别为列。
   0   1   2
0 45   4   3	# 实际(45+4+3)个0类别，识别出45个0,4个识别为1类,3个识别为2类
1 11  56   2
2  5   6  49
（对角矩阵表示预测完全吻合，主对角线上数字越大说明误差越小）
sklearn.metrics.confusion_matrix(实际输出，预测输出)
    --> return：混淆矩阵
查准率precision: 主对角线元素 / 所在列总和
召回率recall：主对角线元素 / 所在行总和

6. 分类报告						 (sklearn.metrics)
分类报告
sklearn.metrics.classification_report(实际输出，预测输出)
    --> return：分类报告


六、随机森林分类器
1. 评估汽车档次
代码：car-randomForest.py

2. 验证曲线--- 训练参数：max_depth，n_estimators......  (sklearn.model_selection)
f1_score = f(模型对象超参数)
验证曲线的峰值，寻找相对理想的超参数。
model = se.RandomForestClassifier(max_depth=8, n_estimators=200, random_state=7)
...
model = se.RandomForestClassifier(max_depth=8, random_state=7)

sklearn.model_selection.validation_curve(model,
                                         x,
                                         y,
                                         'n_estimators',
                                         [100, 200, 300, ...],
                                         cv=5)
    --> return:
        1   2   3   4   5  （交叉验证次数）
100 --> 0.7 0.9 .6 0.8 0.7
200 -->
300 -->
   ...
生成train_score和test_score后绘制曲线，结合训练集和测试集的曲线，确定较佳的参数选择
通常，训练集得分很高，测试集得分很低，说明模型过拟合，即训练集特征匹配很好，但不太适合测试集，模型不够泛化

def validation_curve(estimator, X, y, param_name, param_range, groups=None,
                     cv=None, scoring=None, n_jobs=1, pre_dispatch="all",
                     verbose=0):
    param_name : string
        Name of the parameter that will be varied.
    param_range : array-like, shape (n_values,)
        The values of the parameter that will be evaluated.

3. 学习曲线--- 训练参数：train_sizes		   (sklearn.model_selection)
f1_score = f(训练集大小)
sklearn.model_selection.learning_curve(model,
                                       x,
                                       y,
                                       训练集大小数组,
                                       cv=5)
    --> return: 训练集大小数组，训练集得分矩阵，测试集得分矩阵
代码：learnCurve_Test.py


***********************************支持向量机****************************************
七、支持向量机（SVM）

1. 分类边界
同时满足四个条件：
    A. 正确分类
    B. 支持向量到分类边界的距离相等
    C. 间距最大
    D. 线性（直线，平面）
2. 升维变换（如二维不可分 --> 三维可分）
对于在低纬度空间中无法线性划分的样本，通过升维变换，在高纬度空间寻找最佳线性分类边界。
核函数：用于对特征值进行升维变换的函数
        线性核函数   linear
	多项式核函数 poly
	径向基核函数 rbf	 ？？？？？原理是啥？？？？、
代码：svm_line.py, svm_poly.py
3. 当不同类别的样本数量相差悬殊时，样本数较少的类别可能被支持向量机分类器忽略，为此可以通过class_wieght参数指定为balanced， 通过调节不同类别样本的权重，均衡化。
model = svm.SVC(kernel='linear', class_weight='balanced')

4. 置信概率
svm.SVM(..., probability=True,..)   
支持向量机分类器.predict_proba(输入样本) --> 置信概率矩阵
	类别1    类别2
样本1 -> 0.99	  0.01
样本2 -> 0.02	  0.98


*********************************20180802********************************************
5. 最优超参数
sklearn.model_selection.GridSearchCV(模型,参数组合表,cv=交叉验证次数)
	--> return: 最优模型对象
参数组合表：[{参数名:[取值列表],{},...}]
代码：bhp.py

事件预测
代码：svm_evtCase.py
利用支持向量机回归模型预测交通流量
model = svm.SVR(kernel='rbf', C=10)		# 不是svm.SVC classifier ,SVR:Regressor
svm_traffic.py



******************************************************************************************************************
						聚类
vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
八、聚类
1. K均值 KMeans --- 基于点与点之间的距离（均值距离）
根据事先给定的聚类数，为每个聚类随机分配中心点，计算所有样本与各个中心点的距离，将每个样本分配到与其距离最近的中心点所在的聚类中。
计算每个聚类的几何中心(各个聚类取平均值)，用该几何中心作为新的聚类中心，重新划分聚类。直到计算出的几何中心与上一次聚类使用的聚类中心重合或者足够接近为止。
缺点：
聚类数必须事先已知：从业务中找，选择最优化的指标。
聚类结果会受样本比例的影响。
聚类中心的初始位置会影响聚类的结果（可能只收敛到次优解，类似于非凸函数的梯度下降收敛问题）。
代码：Cluster_KMeans.py
图像量化
代码：quant.py

2. 均值漂移 MeanShift --- 基于点的分布密度（概率密度函数）
把训练样本看成服从某种概率密度函数规则的随机分布，在不断迭代的过程中试图寻找最佳的模式匹配，该密度函数的峰值点就是聚类的中心，被该密度函数所覆盖的样本即隶属于该聚类。
相比于k均值的优点：
不需要事先给定聚类数，算法本身具有发现聚类数量的能力。
代码：Cluster_MeanShift.py

k均值和均值漂移都属于有中心聚类
凝聚层次属于无中心聚类

3.凝聚层次
凝聚层次聚类，可以是自下而上（聚），也可以是自上而下（分）的。
在自下而上的算法中，每个训练样本都被看做是一个单独的集群，根据样本之间的相似度，将其不断合并，直到集群数达到事先指定的聚类数为止。
在自上而下的算法中，所有训练样本被看做是一个大的聚类，根据样本之间的差异度，将其不断拆分，直到集群数达到指定的聚类数为止。
代码：agglo.py
凝聚层次算法，不同于其他基于中心的聚类算法，用他对一些在空间上具有明显连续性，但彼此间的距离未必最近的样本，可以优先聚集，这样所构成的聚类划分就能够表现出较强的连续特性。
代码：spiral.py

4. DBSCAN
"朋友的朋友也是朋友"
从任何一个训练样本出发，以一个事先给定的半径做圆，凡是不在此圆之外的样本都与圆心样本同类，再以这些同类样本为圆心做圆重复以上过程，直到没有新的同类样本加入到该聚类为止。以此类推，获得样本空间中的所有聚类。那些不属于任何聚类的样本，被称为偏离样本；位于聚类边缘的样本，则称为外周样本，其余统一称为核心样本。


5. 轮廓系数 （聚类的评价指标）
表示聚类划分内密外疏的程度。
轮廓系数有以下两个指标构成：
a：一个样本与其所在聚类中其他样本的平均距离
b：一个样本与其距离最近的另一个聚类中样本的平均距离
针对这一个样本的轮廓系数：
s = (b - a)/max(a, b)
针对一个数据集，其轮廓系数就是其中所有样本的轮廓系数的平均值。
轮廓系数的值介于[-1,1]区间，1表示完美聚类，-1表示错误聚类，0表示聚类重叠。
代码：Cluster_score.py


*********************************20180803********************************************
*****************************************************************************************************************
						推荐引擎
vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
九、推荐引擎
1. 管线
-输入 -> 学习模型1 -> 输出 -> 学习模型2 -> 输出 -> ...
代码：map.py  reduce.py

		
	      输入
	       |
	       V
def 学习模型1(参数)：
    ...
    return 返回值
	       |	
	       V	
def 学习模型2(参数)：
    ...
    return 返回值
	     |	
	     V	
	    输出
所谓管线，其本质就是函数的级联调用，即用一个函数的返回值作为另一个函数的参数
代码：cc.py

特征选择器 -- 随机森林分类器
代码：pipe.py

2. 选择最近邻（FNN）--- 推荐引擎常用 -- 只能有监督，不能无监督

sn.NearestNeighbors(n_neighbors=邻居数,
                     algorithm=算法,              # 'ball_tree'
                     )
    --> return: FNN模型
                FNN模型.fit(已知样本集合)
                            xxx...xxx   --> row_11
                            ...
                            xxx...xxx   --> row_23
                            ...
                FNN模型.kneighbors(待求样本集合)
                                    xxx...xxx  11 23 34 (近邻下标索引) 0.5 0.3 0.1 (近邻距离)
                                    xxx...xxx  22 10 15 (近邻下标索引) 0.4 0.2 0.1 (近邻距离)
                                    ...
                    --> return: 与待求样本近邻的多个已知样本与待求样本的距离矩阵
                                与待求样本近邻的已知样本下标索引矩阵
代码：FNN.py

3. KNN分类和回归
遍历训练集中所有样本，计算每个样本与待测样本的距离，并从中挑选出k的最近邻（FNN）。根据与距离成反比的权重，做加权投票（分类）或平均（回归），得到待测样本的类别标签或预测值。
note: 增加了距离权重进行分类或回归
局限性：对于训练集范围之外的样本 无法进行分类和回归



4. 欧氏距离（欧几里得距离） ---------  相似度
(x1,y2) <----> (x2,y2)
    _________________________________________________
   /
 \/ (x1-x2)^2 + (y1-y2)^2 + (z1-z2)^2 + ... + (a,b,c)

                      1
欧氏距离得分 = ---------------
                 1 + 欧氏距离
[0 <-- 不想似- 欧氏距离得分- 相似 --> 1]
代码：es.py
得分矩阵：
	用户1   用户2   用户3 ...
用户1	1	0.8	0.9 ...
用户2	0.8	1	0.7 ...
用户3	...
...
欧几里得距离 Euclidean Distance： 需要自己计算
缺点：计算的是绝对距离，没有考虑不同样本score的标准不同


5. 皮(尔逊)氏距离得分	-----------  相似度
用两个样本的协方差（[-1, 1]）表示相似度。(相比于欧氏距离得分，避免了基准不同和波动性的影响)
     A   B   C
1    5   1   3
2    10  0   5


根据样本的相似度排序
代码：sim.py

针对每个用户的推荐列表
代码：Recommend.py


*********************************20180806********************************************
十、文本分析
import nltk  -- 自然语言工具包
1. 分词
从完整的文章或段落中，划分出若干独立的语义单元，如句子或者词。
代码：textAnanlysis.py

2. 词干提取
从单词中提取主要成分，未必是合法的词汇。
代码：

3. 词型还原
从名词或动词中抽取原型成分，依然保证其合法性。

4. 词袋模型    --  只考虑单词出现的频次 不考虑顺序 (文本-->数字化特征)
the brown dog is running
the black dog is in the black room
running in the room is forbidden
--------------------------------------------------------------
词袋矩阵:
    the brown dog is running black in room forbidden
1    1   1     1   1  1        0    0  0    0
2    2   0     1   1  0        2    1  1    0
3    1   0     0   1  1        0    1  1    1

5. 词频
  单词在句子中出现的次数
--------------------------
     句子中的总单词数
代码：term_frequency.py

6. 词频逆文档频率（TF-IDF）  term frequencyCinverse document frequency

评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度
			        总样本数 
词频 * 逆文档频率 = 词频 * --------------------
			    包含该单词的样本数
模型思路：如果词w在一篇文档d中出现的频率高，并且在其他文档中很少出现，则认为词w具有很好的区分能力，适合用来把文章d和其他文章区开来。
某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。
缺点：IDF的简单结构并不能有效地反映单词的重要程度和特征词的分布情况（没考虑单词位置信息），使其无法很好地完成对权值调整的功能，所以TFIDF法的精度并不是很高
代码：tfidf.py
文本分类，核心问题预测
xxxxxxxxxxxxx -> 加解密
xxxxxxxxxxxxx -> 摩托车
xxxxxxxxxxxxx -> 棒球
...
--------------------------
xxxxxxxxxxxxx -> ?

代码：doc.py

conda install nltk
import nltk
nltk.download()

性别识别
feature 
do -> male
my -> female
...
om -> male
...
代码：Classify_genderPredic_nltk.py

情绪分类：
xxx   xxx   xxx   ...   xxx
True  False False ...   True --> Positive
代码：Classify_sentmentalRecognize_nltk.py

主题词抽取：
import gensim.models.ldamodel
LDA: Latent Dirichlet Allocation
隐含狄利克雷分布
代码：topic.py

十一、音频分布
1. 模拟音频 和 数字音频
声带 -> 机械振动 --> 频率 + 响度 =>  声场强度 = f(时间)
		        			  |
	           				  v
        耳朵 <- 播放器件机械振动 <- 电压/电流 = f(时间)
						  | A/D 模数转换
						  v
	 .wav文件 <- 存储 <- 数字音频 <- 量化 <- 采样
				|
				v
			       传输
				|
				v
			     回放软件
				| D/A 数模转换
				v	
		     电压/电流 = f(时间) -> 播放器件机械振动 -> 耳朵

2. 借助傅里叶变换提取频率特征
代码：signal.py

3. 在频率特征的基础上结合语音的特点选择主要成分---MFCC，梅尔频率倒谱系数
MFCC矩阵：
	    关键频率1	关键频率2   关键频率3 ...
时域区间1	30	    40		20 ...	--> apple
时域区间2	10	    20		50 ...	--> apple
时域区间3	40	    30	 	60 ...  --> apple
...
代码：mfcc.py

4. 语音识别
HMM：隐马尔科夫模型 			***********重要***************
音频样本 -> MFCC -> HMM -> 标签    HMM数学滤波
代码：spch.py

语音 -> 数字音频流 -> MFCC -> 学习模型 -> 文本 -> TF-IDF -> 模型 -> 语义 -> 应答 ->语音
|<------------------语音识别----------------->| <----自然语言处理------>|<--语音合成-->|
					      |				|

*********************************20180807********************************************
十二、图像识别 
机器学习的图像识别处理大多数针对灰度图处理更方便
1. 机器视觉工具包：OpenCV-Python (开源计算机视觉 Open Source Computer Vision Library)
代码：ImagRecgn_basic.py

2. 边缘检测 （外观特征，二维卷积）
颜色梯度阈值设置决定边缘检测灵敏度    Canny边缘检测

3. 通过均衡直方提升亮度
代码：ImagRecgn_eq.py

GUI:图形用户界面Graphical User Interface

4. 角点检测 哈里斯角点检测
代码：ImagRecgn_corner.py

5. star特征检测

6. sift特征检测
通常先用star特征检测，然后用sift特征检测对star特征点进行筛选

7. 特征描述矩阵

8. 图片识别 （对比语音识别 HMM模型）
代码：ImagRecognize_HMM.py


十三、人脸识别
1. 视频捕捉
代码;FaceRecognize_videoCapture.py

2. 人脸定位
基于哈尔级联分类器的人脸定位

3. 人脸识别
基于OpenCV的局部二值模式直方图（LBPH）模型（降维降噪）
代码：FaceRecognize_LBPH.py

十四、成分分析(CA)
1. 主成分分析（PCA）   降维 筛选出主要特征
主成分分析就是把协方差矩阵做一个奇异值分解，求出最大的奇异值的特征方向。
代码：testPCA_np.py

2. sklearn的PCA接口
N -> K (K < N)
import sklearn.decomposition as dc
model = dc.PCA(K)
pca_x = model.fit_transform(x)	# 一步完成
model.fit(x) # U_reduce	主成分特征矩阵
pca_x = model.transform(x) # Z 降维样本
ipca_x = model.inverse_transform(pca_x) # X_approx 恢复到均值极差转换之前的样本
model.explained_variance_ratio_.sum()  --> 还原率 [0, 1]
0 <--- 还原率 ---> 1
误差越大     误差越小

3. 主成分分析在人脸识别中的应用
代码：face1-5.py

4. 核主成分分析（KPCA）
核函数：升维 |
	     | ---> KPCA
PCA：降维    |
对于在n维空间不可线性分割的样本，通过核函数升维到更高维度空间，再通过主成分分析，在投射误差最小的前提下，降维到n维空间，即寻找可线性分割的投影面，达到简化分类模型的目的。
代码：kpca.py

十五、神经网络
1. 神经元
权重：过滤输入信息，针对不同的数据提高或者降低其作用和影响
      [w1, w2, w3, ..., wn]
偏置：当没有任何输入时的输出，b -- 标量
激活函数：将线性的连续的输入转换为非线性的离散的输出。sigmod / tanh / relu... 增加非线性特征

2. 层
每一层可以由1个到多个神经元组成，层中的神经元接收上一层的输出，并未下一层提供输入。数据只能在相邻层之间传递，不能跨层传输。
3. 多层神经网络
输入层：接收输入样本的各个特征，传递给第一个隐藏层，本身不对数据进行运算。
隐藏层：0到多个，通过权重、偏置和激活函数，对所接收到来自上一层的数据进行运算：O = f(I * W + b)
输出层：功能和隐藏层相同，将计算的结果作为输出的每一个特征。
若隐藏层的层数多于1层，则可以被称为深度神经网络，通常使用的深度神经网络，其隐藏层数都可以多达数十甚至上百层，基于这样结构的学习模型，被称为深度学习。

4. 最简单的神经网络：感知器
只由输入层和输出层组成的神经网络。
代码：neuron.py 神经元

5. 单层多输出神经网络
代码：mono.py

6. OCR字符识别
代码： ocrdb.py   orc.py
	o m a n d i g
a	0 0 1 0 0 0 0
m	0 1 0 0 0 0 0
g	0 0 0 0 0 0 1

sequential：tail10
ommandingo：head10

十六、推荐书目
入门：
Python数据分析基础教程Numpy学习指南，张驭宇译 人民邮电出版社
Python神经网络编程  林赐译，人邮
白话深度学习与TensorFlow  高扬著  机械
基础：
scikit-learn机器学习：常用算法原理和编程实践， 黄永昌主编  机械
机器学习算法原理与编程   郑捷著 电子
进阶：
深度学习  张鹏主编 电子
TensorFlow机器学习项目实战 姚鹏鹏译  人邮
休闲：
数学之美  吴军 人邮
终极算法  黄芳萍译
深度学习   伊恩古德弗洛著  人邮   ----墙裂
 









