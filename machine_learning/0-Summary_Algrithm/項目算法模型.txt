为什么选择这4个模型：
	不同类型的模型：
		svm是小样本学习方法，对小样本数据集效果比较好（大规模时m阶矩阵计算量大，对缺失值、核函数敏感--选择rbf径向基核函数）
		xgboost：集成思想的分类，本身就具备防止过拟合的措施
		bp: 浅层神经网络
		cnn:深层神经网络，数据集是视频谱图 （其他几个输入都是提取的弱关联性的物理特征）

样本怎么处理？
	人工打缺陷，正负样本比例基本一致
	后期在线的模型的数据的正负样本时根据每次的样品抽检良品率来确定
	会对不均衡样本进行处理？under_sampling or smote算法(oversampling method) or 正样本分成k组，与负样本组成1:1的k组数据及
	SMOTE算法相当于从少数样本中选出每一个样本的k个近邻，将这些近邻增加一个随机扰动项生成新的近邻点，增加到保持正负样本比例均衡。
	

（1）前期的模型验证都是采用人工故障后的数据，故障很明显并且是单一故障，实际生产中往往是复合故障；
（2）而且，实际生产中，如果故障已经很明显，说明已经到了很严重的地步，更重要的应该是针对早期轻微故障的诊断和预警，这才是符合生产的场景；
（3）另外，生产中的标定数据获取周期长，主要是现场配合度不高
	标定数据是要根据一段时间内的良品率作为依据来标定轴承是否故障，
	然后通过传统的数字信号分析确定故障类型，才能实现故障类型的标定，
	有时候可能还需要随机取样拆解轴承来确认标定是否正确，所以在线的故障检测其实难度挺大。


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> xgboost <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
1. xgboost
选择原因：
	(1) xgb本身有很多防止过拟合的措施，比如正则项、权重缩减(类似学习速率)、列采样等
	(2) xgb对缺失值不敏感，能够自动学习缺失值样本的分裂方向
	(3) 在特征粒度上可以实现并行化计算，效率高
	
调优顺序：xgb.cv函数调参
	1) 选择较高的学习速率(learning rate),确定对应的理想的决策树数量。
	一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。

	2) 对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。

	3) xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。

	4) 降低学习速率 增加决策树数量，确定理想参数。

其他：见word文档



>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> svm <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2. svm
选择原因：
	svm在小样本数据集上比较适用，模型泛化能力好
	svm的损失函数是凸函数，不存在局部最优解

缺点：
	大规模数据及不太适用

损失函数：
	合页损失函数 hinge loss function + 正则项
	
核函数：rbf(高斯径向基核函数)
		将原始样本从低纬不可分特征空间映射到高纬可分特征空间，实现高纬空间线性可分
		（同时引入拉格朗日对偶问题实现高纬样本维度计算转换为低纬样本数量，优化了算法复杂度，并且没有改变最优解）
		
主要参数：
	C: 误分类的惩罚
	kernel: rbf， (default='rbf')
	gamma:  Kernel coefficient, default is 'auto' which uses 1 / n_features
	probability: Whether to enable probability estimates.
	decision_function_shape: 'ovo', 'ovr', default='ovr'
		'ovo': 需要 C(2,k)个二分类器, 通过voting的方式得到分类结果；     
		'ovr': 需要 k 个二分类器，k个分类器中概率最高的类就是所属的类。 ---- bias高
		
参数调优：
	网格搜索
		
评估指标：
	准确率  (AUC 混淆矩阵 都可以)
	
模型持久化：
	from sklearn.externals import joblib
	# 持久化
	joblib.dump(lr, "result/data_lr.model")
	# 加载
	lr3 = joblib.load("result/data_lr.model")
	
	
	
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> bp <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
3. 浅层神经网络 -- BP神经网络
网路结构：
	3层：input_layer, one-hidden-layer, output_layer
	input_layer的神经元个数为特征提取筛选的特征：11个
	hidden_layer的神经元个数： 2 * 11 + 1 = 23 个
	output_layer: 2 (两种类别)

激活函数：
	relu函数 (sigmoid [0,1], tanh [-1,1])
	sigmoid和tanh：软饱和性，软饱和区向底层传递的梯度会变得非常小，网络参数很难训练到，产生梯度消失；
	relu: 硬饱和区(y=0|x<0), 梯度为0，无法更新，神经元死亡；x>0时梯度=1，不会出现梯度消失的情况

损失函数：
	softmax_cross_entropy_with_logits_v2： 最后一层的线性输出通过softmax运算，然后计算交叉熵

主要参数：
	神经元个数

优化方法：
	反向传播过程中使用的优化方法是 adam 
	(优化算法：GD, Bath-GD, miniBath-GD, Random-GD, momentum-动量梯度, RMSprop, adam)
		- Momentum优化器的主要思想就是 基于梯度的移动指数加权平均 来对网络的参数进行平滑处理的，让梯度的摆动幅度变得更小。
		- RMSProp算法对梯度计算了微分平方加权平均数, 在更新参数时相当于对梯度做了归一化处理，能够使各个方向的梯度震荡都变小
		- adam算法相当于先把原始梯度做一个指数加权平均，再做一次归一化处理，然后再更新梯度值。

评估指标：
	准确率， 混淆矩阵
	


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> cnn <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
4. 深层神经网络 -- 卷积神经网络
思路：cnn主要处理图像的，采集后的是数字信号，通过短时傅里叶变换获得时频谱图像数据集，图像进行剪切和压缩后作为cnn的输入
filter：卷积核
padding: 池化

CNN结构：
	Fist part: Conv2D-->Conv2D-->maxpool2D-->Dropout
				Conv2D	 : 	filters=32, size=(5, 5), padding='SAME', activation='relu'
				maxpool2D:  pool_size=(2,2)
				Dropout  :  rate=0.25	--> 随机失活，避免过拟合
	Second part: Conv2D-->Conv2D-->maxpool2D-->Dropout
				Conv2D	 : 	filters=64, size=(3, 3), padding='SAME', activation='relu'
				maxpool2D:  pool_size=(2,2)
				Dropout  :  rate=0.25	--> 随机失活，避免过拟合
	Third part: Flatten-->FC(Dense layer)-->Dropout--softmax-->output
				FC1： 全连接层1 (256个神经元)， activation='relu'
				Dropout： rate=0.5
				FC2:  全连接层2 (2个神经元--类别)， activation='softmax'

代价函数：
	categorical_crossentropy： 分类交叉熵损失函数

代价函数优化器：
	adam

学习率衰减：
	有助于梯度下降过程的收敛
	
评价指标：
	准确率，混淆矩阵
	







