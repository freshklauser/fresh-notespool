为什么选择这4个模型：
	不同类型的模型：
		svm是小样本学习方法，对小样本数据集效果比较好（大规模时m阶矩阵计算量大，对缺失值、核函数敏感--选择rbf径向基核函数）
		xgboost

样本怎么处理？
	人工打缺陷，正负样本比例基本一致
	后期在线的模型的数据的正负样本时根据每次的样品抽检良品率来确定
	会对不均衡样本进行处理？under_sampling or smote算法(oversampling method) or 正样本分成k组，与负样本组成1:1的k组数据及
	SMOTE算法相当于从少数样本中选出每一个样本的k个近邻，将这些近邻增加一个随机扰动项生成新的近邻点，增加到保持正负样本比例均衡。
	

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> xgboost <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
1. xgboost
选择原因：
	(1) xgb本身有很多防止过拟合的措施，比如正则项、权重缩减(类似学习速率)、列采样等
	(2) xgb对缺失值不敏感，能够自动学习缺失值样本的分裂方向
	(3) 在特征粒度上可以实现并行化计算，效率高
	
调优顺序：xgb.cv函数调参
	1) 选择较高的学习速率(learning rate),确定对应的理想的决策树数量。
	一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。

	2) 对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。

	3) xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。

	4) 降低学习速率 增加决策树数量，确定理想参数。

其他：见word文档



>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> svm <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
2. svm
选择原因：
	svm在小样本数据集上比较适用，模型泛化能力好
	svm的损失函数是凸函数，不存在局部最优解

缺点：
	大规模数据及不太适用

损失函数：
	合页损失函数 hinge loss function + 正则项
	
核函数：rbf(高斯径向基核函数)
		将原始样本从低纬不可分特征空间映射到高纬可分特征空间，实现高纬空间线性可分
		（同时引入拉格朗日对偶问题实现高纬样本维度计算转换为低纬样本数量，优化了算法复杂度，并且没有改变最优解）
		
主要参数：
	C: 误分类的惩罚
	kernel: rbf， (default='rbf')
	gamma:  Kernel coefficient, default is 'auto' which uses 1 / n_features
	probability: Whether to enable probability estimates.
	decision_function_shape: 'ovo', 'ovr', default='ovr'
		'ovo': 需要 C(2,k)个二分类器, 通过voting的方式得到分类结果；     
		'ovr': 需要 k 个二分类器，k个分类器中概率最高的类就是所属的类。 ---- bias高
		
参数调优：
	网格搜索
		
评估指标：
	准确率  (AUC 混淆矩阵 都可以)
	
模型持久化：
	from sklearn.externals import joblib
	# 持久化
	joblib.dump(lr, "result/data_lr.model")
	# 加载
	lr3 = joblib.load("result/data_lr.model")
	
	
	
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> bp <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
3. 浅层神经网络 -- BP神经网络
网路结构：
	3层：input_layer, one-hidden-layer, output_layer
	input_layer的神经元个数为特征提取筛选的特征：11个
	hidden_layer的神经元个数： 2 * 11 + 1 = 23 个
	output_layer: 2 (两种类别)

激活函数：
	relu函数 (sigmoid [0,1], tanh [-1,1])
	sigmoid和tanh：软饱和性，软饱和区向底层传递的梯度会变得非常小，网络参数很难训练到，产生梯度消失；
	relu: 硬饱和区(y=0|x<0), 梯度为0，无法更新，神经元死亡；x>0时梯度=1，不会出现梯度消失的情况

损失函数：
	softmax_cross_entropy_with_logits_v2： 最后一层的线性输出通过softmax运算，然后计算交叉熵

主要参数：
	神经元个数

优化方法：
	反向传播过程中使用的优化方法是 adam 
	(优化算法：GD, Bath-GD, miniBath-GD, Random-GD, momentum-动量梯度, adam)

评估指标：
	准确率， 混淆矩阵
	


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> cnn <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
4. 深层神经网络 -- 卷积神经网络
思路：cnn主要处理图像的，考虑将数字转化为图像的格式，然后按照图像的方法来处理
filter：卷积核
padding: 池化

CNN结构：
	Fist part: Conv2D-->Conv2D-->maxpool2D-->Dropout
				Conv2D	 : 	filters=32, size=(5, 5), padding='SAME', activation='relu'
				maxpool2D:  pool_size=(2,2)
				Dropout  :  rate=0.25	--> 随机失活，避免过拟合
	Second part: Conv2D-->Conv2D-->maxpool2D-->Dropout
				Conv2D	 : 	filters=64, size=(3, 3), padding='SAME', activation='relu'
				maxpool2D:  pool_size=(2,2)
				Dropout  :  rate=0.25	--> 随机失活，避免过拟合
	Third part: Flatten-->FC(Dense layer)-->Dropout--softmax-->output
				FC1： 全连接层1 (256个神经元)， activation='relu'
				Dropout： rate=0.5
				FC2:  全连接层2 (2个神经元--类别)， activation='softmax'

代价函数：
	categorical_crossentropy： 分类交叉熵损失函数

代价函数优化器：
	adam

学习率衰减：
	有助于梯度下降过程的收敛
	
评价指标：
	准确率，混淆矩阵
	







