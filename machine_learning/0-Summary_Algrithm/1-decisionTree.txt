>>>>> 后续整理成word文档
信息熵：Information Entropy
信息增益：Information Gain
信息增益率：Information Gain Ratio


1. 构造树的基本思想：
	随着树深度的增加，节点的熵迅速地降低。熵降低的速度越快越好，这样就有望得到一颗高度最矮的决策树。
	
2. 	决策树三要素：特征选择，决策树生成，决策树剪枝
	refer: https://www.cnblogs.com/muzixi/p/6566803.html


3. 决策树是如何做特征选择？
	refer: https://www.cnblogs.com/muzixi/p/6566803.html
	决策树特征选择的准则对比：ID3算法, C4.5算法, CART算法
	构建决策树依据：
		ID3 : 信息增益
		C4.5: 信息增益比率
		CART: Gini系数
	
	假设初始熵为0.94
	1) 信息增益：
		比如对于ID编号(唯一)，以ID=1为根节点时，将ID=1的划分后的节点中只有ID=1的数据，此时该节点中P[id=1]=1;
			对于ID=i的特征划分后的节点，P[id=i]=1始终成立；
			则以ID特征划分后的熵=sum(-1*log(1)) = 0, 此时信息增益Gain=初始熵-划分后的熵=0.94-0=0.94， 信息增益最大化了。
			因此以ID特征作为根节点划分能使信息增益最大，但是显然ID特征与分类并没有关系，因此以信息增益为构建决策树依据时需避免这种情况。
		--> 即，对于特征的属性很多，并且该特征的每个属性对应的样本个数又很少的情况，会出现上述问题
		--> 引入 信息增益率 解决上述问题
	2) 信息增益率：
	
	3) Gini系数
	
4. 预剪枝和后剪枝
	预剪枝：在构建决策树的过程时，提前停止
	后剪枝：在决策树构建完成之后再开始剪枝，根据如下评价函数 C表示gini或gain等值
			C_a(T) = C(T) + a*T_leaf  叶子节点个数越多，损失越大
			
			

5. 采样
	Bootstraping: 有放回采样
	Bagging: 有放回采样n个样本，一共建立分类器k个
	
6. 随机森林
	随机： ---> 防止过拟合
		1) 样本随机选择：随机选择一个比例的数据样本(有放回采样)
		2) 特征随机选择：
		
