一、原理
---> 原理及推导可参考 https://www.cnblogs.com/kuangsyx/p/9043168.html， 描述的还算详细
	---> 数学模型:     y_pred_i = ∑(1,K)f_k(x_i),  k-->[1,K]
	---> 损失函数:     loss_function = (y^i−yi)^2 (可自定义其他loss func， 前提是lossfunc 一阶和二阶可导)
	---> 加法训练过程：y_pred{t} = y_pred{t-1} + f_t(x_i)   f_t(x_i) 为每次的新增函数
	---> 模型正则项：  Omiga(f) = γ*T + 1/2*λ*sum(w_j^2)  (j-->[1,T])
		 γ越大，表示越希望获得结构简单的树，因为此时对较多叶子节点的树的惩罚越大。λ越大也是越希望获得结构简单的树
	---> 优化目标：    Obj_t = loss_function + Omiga(f)

1. 简单介绍一下 xgboost(或 xgboost的算法思想)
	基于集成思想，由很多CART回归数集成，通过若干弱分类器的组合成一个强分类器，使模型具有更强的泛化能力。
	具体来说就是不断地通过特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。当训练完k棵树后，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中样本会被到对应的一个叶子节点，每个叶子节点刚好对应一个预测值，将所有这些叶子节点对应的值加起来就是该样本的预测值。
	主要是对GBDT进行了一系列的优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。
	
	--> GBDT: 
	它是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。（每加入一棵树期望整体表达效果更好）
	
	--> xgboost与randomForest的区别：
	randomForest的各个决策树是独立的、每个决策树在样本堆里随机选一批样本，随机选一批特征进行独立训练; 而xgboost的决策树依赖于前面决策树的训练和预测结果。

2. xgboost的节点是如何分裂的（或xgboost是怎么寻找最佳特征，怎么寻找最佳分裂点）？
	这个问题本质上是基于空间切分去构造一颗决策树，属于NP难问题，求全局最优解的代价太大。针对这个问题，xgboost采用了贪心算法，

	
3. XGBoost与GBDT有什么不同？
    (1)基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。
    (2)导数信息：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。
    (3)正则项：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。
    (4)列抽样：XGBoost支持列采样，与随机森林类似，用于防止过拟合。
    (5)缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。 (6)并行化：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。
	
	--> XGBoost为什么使用泰勒二阶展开?
	(1)可以更为精准的逼近真实的损失函数;
	(2)可扩展性:损失函数支持自定义，只需要新的损失函数二阶可导
	
4. 损失函数
	L(ϕ)=∑i L(y^i−yi)+ ∑k Ω(fk)
	其中：L(y^i−yi): 样本 xi 的训练误差
		  Ω(fk)表示第k颗树的正则项。