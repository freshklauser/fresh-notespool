一、原理
1. 简单介绍一下 xgboost
	基于集成思想，由很多CART回归数集成，使组合后的模型具有更强的泛化能力。
	主要是对GBDT进行了一系列的优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。
	
	--> GBDT: 
	它是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。
	
	--> xgboost与randomForest的区别：
	randomForest的各个决策树是独立的、每个决策树在样本堆里随机选一批样本，随机选一批特征进行独立训练; 而xgboost的决策树依赖于前面决策树的训练和预测结果。
	
	
2. XGBoost与GBDT有什么不同？
    (1)基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。
    (2)导数信息：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。
    (3)正则项：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。
    (4)列抽样：XGBoost支持列采样，与随机森林类似，用于防止过拟合。
    (5)缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。 (6)并行化：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。
	
	--> XGBoost为什么使用泰勒二阶展开?
	(1)可以更为精准的逼近真实的损失函数;
	(2)可扩展性:损失函数支持自定义，只需要新的损失函数二阶可导
	
3. 损失函数
	L(ϕ)=∑i L(y^i−yi)+ ∑k Ω(fk)
	其中：L(y^i−yi): 样本 xi 的训练误差
		  Ω(fk)表示第k颗树的正则项。